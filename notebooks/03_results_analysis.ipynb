{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010c3a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import load_model\n",
    "from evaluation import ModelEvaluator\n",
    "from data_preprocessing import MovieDataPreprocessor\n",
    "\n",
    "# Cargar datos de prueba\n",
    "DATA_PATH = \"../data/raw\"\n",
    "preprocessor = MovieDataPreprocessor(DATA_PATH)\n",
    "X, y_labels = preprocessor.load_data()\n",
    "y_categorical, y_encoded = preprocessor.preprocess_labels(y_labels)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = preprocessor.create_data_splits(X, y_categorical)\n",
    "\n",
    "# Cargar modelos entrenados\n",
    "cnn_model = load_model('../data/models/cnn_best.h5')\n",
    "mlp_model = load_model('../data/models/mlp_best.h5')\n",
    "\n",
    "print(\"‚úÖ Modelos y datos cargados correctamente\")\n",
    "\n",
    "# Celda 2: Evaluaci√≥n detallada\n",
    "evaluator = ModelEvaluator(preprocessor.label_encoder)\n",
    "\n",
    "print(\"üìä EVALUACI√ìN DETALLADA DE MODELOS\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# Evaluar CNN\n",
    "cnn_metrics, cnn_report, cnn_pred, cnn_proba = evaluator.evaluate_model(cnn_model, X_test, y_test)\n",
    "print(\"\\nüî• Resultados CNN:\")\n",
    "for metric, value in cnn_metrics.items():\n",
    "    print(f\"  {metric.title()}: {value:.4f}\")\n",
    "\n",
    "# Evaluar MLP\n",
    "mlp_metrics, mlp_report, mlp_pred, mlp_proba = evaluator.evaluate_model(mlp_model, X_test, y_test)\n",
    "print(\"\\nüß† Resultados MLP:\")\n",
    "for metric, value in mlp_metrics.items():\n",
    "    print(f\"  {metric.title()}: {value:.4f}\")\n",
    "\n",
    "# Celda 3: An√°lisis de errores\n",
    "def analyze_prediction_errors(y_true, y_pred, y_proba, model_name):\n",
    "    \"\"\"Analizar errores de predicci√≥n\"\"\"\n",
    "    print(f\"\\nüîç AN√ÅLISIS DE ERRORES - {model_name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Encontrar predicciones incorrectas\n",
    "    wrong_predictions = np.where(y_true != y_pred)[0]\n",
    "    confidence_scores = np.max(y_proba, axis=1)\n",
    "    \n",
    "    print(f\"Predicciones incorrectas: {len(wrong_predictions)}/{len(y_true)} ({len(wrong_predictions)/len(y_true)*100:.1f}%)\")\n",
    "    \n",
    "    # Errores con alta confianza (falsos positivos seguros)\n",
    "    confident_errors = wrong_predictions[confidence_scores[wrong_predictions] > 0.8]\n",
    "    print(f\"Errores con alta confianza (>80%): {len(confident_errors)}\")\n",
    "    \n",
    "    # Errores con baja confianza\n",
    "    unconfident_errors = wrong_predictions[confidence_scores[wrong_predictions] < 0.5]\n",
    "    print(f\"Errores con baja confianza (<50%): {len(unconfident_errors)}\")\n",
    "    \n",
    "    return wrong_predictions, confidence_scores\n",
    "\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "cnn_errors, cnn_confidence = analyze_prediction_errors(y_true, cnn_pred, cnn_proba, \"CNN\")\n",
    "mlp_errors, mlp_confidence = analyze_prediction_errors(y_true, mlp_pred, mlp_proba, \"MLP\")\n",
    "\n",
    "# Celda 4: Visualizaci√≥n de predicciones incorrectas\n",
    "def plot_wrong_predictions(X_test, y_true, y_pred, y_proba, label_encoder, errors, title, n_samples=8):\n",
    "    \"\"\"Visualizar predicciones incorrectas\"\"\"\n",
    "    selected_errors = np.random.choice(errors, min(n_samples, len(errors)), replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, idx in enumerate(selected_errors):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(X_test[idx])\n",
    "        \n",
    "        true_label = label_encoder.inverse_transform([y_true[idx]])[0]\n",
    "        pred_label = label_encoder.inverse_transform([y_pred[idx]])[0]\n",
    "        confidence = np.max(y_proba[idx]) * 100\n",
    "        \n",
    "        ax.set_title(f'Real: {true_label}\\nPredicci√≥n: {pred_label}\\nConfianza: {confidence:.1f}%', \n",
    "                    fontsize=10)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'{title} - Predicciones Incorrectas', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if len(cnn_errors) > 0:\n",
    "    plot_wrong_predictions(X_test, y_true, cnn_pred, cnn_proba, \n",
    "                          preprocessor.label_encoder, cnn_errors, \"CNN\")\n",
    "\n",
    "if len(mlp_errors) > 0:\n",
    "    plot_wrong_predictions(X_test, y_true, mlp_pred, mlp_proba, \n",
    "                          preprocessor.label_encoder, mlp_errors, \"MLP\")\n",
    "\n",
    "# Celda 5: An√°lisis por clase\n",
    "def analyze_per_class_performance(y_true, y_pred, label_encoder):\n",
    "    \"\"\"An√°lisis de rendimiento por clase\"\"\"\n",
    "    from sklearn.metrics import classification_report\n",
    "    \n",
    "    report = classification_report(y_true, y_pred, \n",
    "                                 target_names=label_encoder.classes_, \n",
    "                                 output_dict=True)\n",
    "    \n",
    "    # Convertir a DataFrame\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    df_report = df_report.iloc[:-3, :-1]  # Remover resumen\n",
    "    \n",
    "    return df_report\n",
    "\n",
    "print(\"\\nüìà RENDIMIENTO POR CLASE\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "cnn_class_perf = analyze_per_class_performance(y_true, cnn_pred, preprocessor.label_encoder)\n",
    "mlp_class_perf = analyze_per_class_performance(y_true, mlp_pred, preprocessor.label_encoder)\n",
    "\n",
    "print(\"\\nCNN - Rendimiento por clase:\")\n",
    "print(cnn_class_perf.round(3))\n",
    "\n",
    "print(\"\\nMLP - Rendimiento por clase:\")\n",
    "print(mlp_class_perf.round(3))\n",
    "\n",
    "# Celda 6: Visualizaci√≥n comparativa final\n",
    "def create_final_comparison_plot(cnn_metrics, mlp_metrics):\n",
    "    \"\"\"Crear gr√°fico comparativo final\"\"\"\n",
    "    metrics = list(cnn_metrics.keys())\n",
    "    cnn_values = list(cnn_metrics.values())\n",
    "    mlp_values = list(mlp_metrics.values())\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    bars1 = ax.bar(x - width/2, cnn_values, width, label='CNN', color='skyblue', alpha=0.8)\n",
    "    bars2 = ax.bar(x + width/2, mlp_values, width, label='MLP', color='lightcoral', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('M√©tricas')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Comparaci√≥n Final de Modelos')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([m.title() for m in metrics])\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Agregar valores en las barras\n",
    "    def autolabel(bars):\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{height:.3f}',\n",
    "                       xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                       xytext=(0, 3),\n",
    "                       textcoords=\"offset points\",\n",
    "                       ha='center', va='bottom')\n",
    "    \n",
    "    autolabel(bars1)\n",
    "    autolabel(bars2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/plots/final_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "create_final_comparison_plot(cnn_metrics, mlp_metrics)\n",
    "\n",
    "# Celda 7: Generaci√≥n de reporte\n",
    "def generate_summary_report(cnn_metrics, mlp_metrics, dataset_info):\n",
    "    \"\"\"Generar reporte resumen\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"REPORTE FINAL DEL PROYECTO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nüìä INFORMACI√ìN DEL DATASET:\")\n",
    "    print(f\"  ‚Ä¢ Total de muestras: {dataset_info['total_samples']}\")\n",
    "    print(f\"  ‚Ä¢ N√∫mero de clases: {dataset_info['num_classes']}\")\n",
    "    print(f\"  ‚Ä¢ Clases: {', '.join(dataset_info['class_distribution'].keys())}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ RESULTADOS FINALES:\")\n",
    "    print(f\"  CNN:\")\n",
    "    for metric, value in cnn_metrics.items():\n",
    "        print(f\"    ‚Ä¢ {metric.title()}: {value:.4f}\")\n",
    "    \n",
    "    print(f\"  MLP:\")\n",
    "    for metric, value in mlp_metrics.items():\n",
    "        print(f\"    ‚Ä¢ {metric.title()}: {value:.4f}\")\n",
    "    \n",
    "    # Determinar mejor modelo\n",
    "    mejor_modelo = 'CNN' if cnn_metrics['accuracy'] > mlp_metrics['accuracy'] else 'MLP'\n",
    "    diferencia = abs(cnn_metrics['accuracy'] - mlp_metrics['accuracy'])\n",
    "    \n",
    "    print(f\"\\nü•á MEJOR MODELO: {mejor_modelo}\")\n",
    "    print(f\"   Diferencia en accuracy: {diferencia:.4f} ({diferencia*100:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nüí° CONCLUSIONES:\")\n",
    "    if diferencia < 0.05:\n",
    "        print(\"   ‚Ä¢ Los modelos tienen rendimiento similar\")\n",
    "        print(\"   ‚Ä¢ La diferencia es menor al 5%\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ El modelo {mejor_modelo} supera significativamente al otro\")\n",
    "    \n",
    "    if cnn_metrics['accuracy'] > mlp_metrics['accuracy']:\n",
    "        print(\"   ‚Ä¢ Las CNNs son m√°s efectivas para an√°lisis de im√°genes\")\n",
    "        print(\"   ‚Ä¢ La extracci√≥n de caracter√≠sticas espaciales es clave\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ El MLP logr√≥ competir con la CNN\")\n",
    "        print(\"   ‚Ä¢ Posible overfitting en la CNN o underfitting\")\n",
    "\n",
    "dataset_info = preprocessor.get_dataset_info(y_categorical)\n",
    "generate_summary_report(cnn_metrics, mlp_metrics, dataset_info)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
